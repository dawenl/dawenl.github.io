<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Selected Projects</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Selected Projects</h1>
</div>
<h2>Nonparametric Bayesian Dictionary Learning for Machine Listening</h2>
<p>2012.10 &ndash; 2013.1 </p>
<p>The course project for the topic course
<a href="http://www.columbia.edu/~jw2966/course-FA12.html">Sparse
Representation and High-dimensional Geometry</a>, in which I extend the
nonparametric Bayesian dictionary learning model with time constrain, so that it
better fits the application for machine listening, which typically deals with
one-dimensional (time-series) sound signal. Gibbs sampler is utilized for
approximate inference. Preliminary experiments show that
it somewhat performs better than the model without time dependency when modeling sound
signal &ndash; a lot of future work is required, but the preliminary results are
promising. </p>
<p><a href="files/E6886_sparse.pdf">Final report</a></p>
<h2>Cross-Modal Music Information Retrieval</h2>
<p>2011.9 &ndash; 2011.12</p>
<p>As the course project of the <a href="http://www.cs.cmu.edu/~christos/courses/826.F11/">Multimedia Databases and Data Mining</a> course, we propose a cross-modal retrieval framework for large-scale music-text data mining. In this project, we try to find an efficient model 
to combine different kinds of music-related features (e.g. acoustic feature and lyrical feature) and evalute our model with some traditional MIR tasks.
Currently for simplicity, we choose
<a href="http://ismir2001.ismir.net/pdf/tzanetakis.pdf">music genre
classification</a> as evaluation task.
But as a future work, we will extend it to
<a href="http://www.columbia.edu/~tb2332/Papers/machineaudition10.pdf">music
autotagging</a> which should not be hard considering the nature of our
modeling. Here we experiment with the
<a href="http://labrosa.ee.columbia.edu/millionsong/">The Million Song
Dataset</a>.</p>
<p><a href="files/FINAL.pdf">Final report</a></p>
<h2>Rehearsal Audio Segmentation and Clustering</h2>
<p>2010.9 &ndash; 2010.12</p>
<p>Based on our course project of the
<a href="http://mlsp.cs.cmu.edu/courses/fall2010/">Machine Learning for Signal
Processing</a> course, we describe a systematic investigation to provide useful capabilities to musicians both in rehearsal and when practicing alone. The goal of this project is: given unordered and noisy raw rehearsal recordings, we would first extract the music segments, then cluster the segments belonging to the same piece together. We proposed	the use of Eigenmusic features and utilized	the Adaboost classifier	to obtain a	result comparable with the state-of-the-art. Moreover, we adopted Hidden Markov Models (HMM) to further smooth the classification result. Finally, we introduce an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and output in terms of conventional music notation.</p>
<p><a href="publications/XiaLDH11-rehearsal.pdf">ISMIR 2011 paper</a></p>
<h2>Human Computer Music Performance (HCMP)</h2>
<p>2010.9 &ndash; 2011.6</p>
<p>Abstract:
Computer music systems that coordinate or interact with human musicians exist in many forms. Often, coordination is at the level of gestures and phrases without synchronization at the beat level (or perhaps the notion of &ldquo;beat&rdquo; does not even exist). In music with beats, fine-grain synchronization can be achieved by having humans adapt to the computer (e.g. following a click track), or by computer accompaniment in which the computer follows a predetermined score. We consider an alternative scenario in which improvisation prevents traditional score following, but where synchronization is achieved at the level of beats, measures, and cues. To explore this new type of human-computer interaction, we have created new software abstractions for synchronization and coordination of music and interfaces in different modalities. We describe these new software structures, present examples, and introduce the idea of music notation as an interactive musical interface rather than a static document.	</p>
<p><a href="publications/LiangXD11-hcmp.pdf">NIME 2011 paper</a></p>
<p><b>Back to <a href="index.html">homepage</a></b></p>
<div id="footer">
<div id="footer-text">
Page generated 2014-03-07 02:17:43 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
